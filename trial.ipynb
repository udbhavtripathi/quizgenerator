{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# pip install git+https://github.com/boudinfl/pke.git\n",
    "\n",
    "# pip install sense2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m nltk.downloader universal_tagset\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Sense2vec wordvectors for generation of multiple choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "\n",
    "url = \"https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"s2v_reddit_2015_md.tar.gz\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"File downloaded successfully.\")\n",
    "    \n",
    "    # Extract the contents of the tar.gz file\n",
    "    with tarfile.open(\"s2v_reddit_2015_md.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "    print(\"File extracted successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download the file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 3829-7CC4\n",
      "\n",
      " Directory of c:\\Users\\Admin\\Desktop\\new\\Questgen.ai-master\\Questgen.ai-master\\s2v_old\n",
      "\n",
      "28-09-2019  16:26    <DIR>          .\n",
      "31-07-2023  16:36    <DIR>          ..\n",
      "18-11-2019  04:04               174 ._cfg\n",
      "28-09-2019  16:26               174 ._freqs.json\n",
      "28-09-2019  16:26               174 ._key2row\n",
      "28-09-2019  16:26               174 ._strings.json\n",
      "28-09-2019  16:26               174 ._vectors\n",
      "18-11-2019  04:04               424 cfg\n",
      "28-09-2019  16:26        49,969,681 freqs.json\n",
      "28-09-2019  16:26        16,492,891 key2row\n",
      "28-09-2019  16:26        26,188,439 strings.json\n",
      "28-09-2019  16:26       611,973,760 vectors\n",
      "              10 File(s)    704,626,065 bytes\n",
      "               2 Dir(s)  117,919,739,904 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir s2v_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genertaions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate boolean (Yes/No) Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from Questgen import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\torch\\serialization.py:975: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  obj = cast(Storage, torch.UntypedStorage(nbytes))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Questgen.main.BoolQGen at 0x1dc1e8bece0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qe= main.BoolQGen()\n",
    "qe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "            \"input_text\": \"Ukraine is a country in Eastern Europe. It is the second-largest European country after Russia, which it borders to the east and northeast. Ukraine covers approximately 600,000 square kilometres (230,000 sq mi). Prior to the ongoing Russo-Ukrainian War, it was the eighth-most populous country in Europe, with a population of around 41 million people. It is also bordered by Belarus to the north; by Poland, Slovakia, and Hungary to the west; and by Romania and Moldova to the southwest; with a coastline along the Black Sea and the Sea of Azov to the south and southeast. Kyiv is the nation's capital and largest city. Ukraine's official and national language is Ukrainian; most people are also fluent in Russian.\",\n",
    "            \"max_questions\": 6\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Boolean Questions': ['Is ukraine the second largest country in europe?',\n",
      "                       'Is ukraine and russia the same country?',\n",
      "                       'Is ukraine a country in eastern europe?',\n",
      "                       'Is ukraine the largest country in europe?',\n",
      "                       'Is ukraine part of the eu?',\n",
      "                       'Is ukraine the second largest country in the world?',\n",
      "                       'Is ukraine the second biggest country in europe?',\n",
      "                       'Is ukraine part of eastern europe?',\n",
      "                       'Is ukraine the largest country in the world?',\n",
      "                       'Is ukraine part of the eu country?'],\n",
      " 'Count': 10,\n",
      " 'Text': 'Ukraine is a country in Eastern Europe. It is the second-largest '\n",
      "         'European country after Russia, which it borders to the east and '\n",
      "         'northeast. Ukraine covers approximately 600,000 square kilometres '\n",
      "         '(230,000 sq mi). Prior to the ongoing Russo-Ukrainian War, it was '\n",
      "         'the eighth-most populous country in Europe, with a population of '\n",
      "         'around 41 million people. It is also bordered by Belarus to the '\n",
      "         'north; by Poland, Slovakia, and Hungary to the west; and by Romania '\n",
      "         'and Moldova to the southwest; with a coastline along the Black Sea '\n",
      "         \"and the Sea of Azov to the south and southeast. Kyiv is the nation's \"\n",
      "         \"capital and largest city. Ukraine's official and national language \"\n",
      "         'is Ukrainian; most people are also fluent in Russian.'}\n"
     ]
    }
   ],
   "source": [
    "output = qe.predict_boolq(payload)\n",
    "pprint (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate MCQ Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from Questgen import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload = {\n",
    "#             \"input_text\": \"Ukraine is a country in Eastern Europe. It is the second-largest European country after Russia, which it borders to the east and northeast. Ukraine covers approximately 600,000 square kilometres (230,000 sq mi). Prior to the ongoing Russo-Ukrainian War, it was the eighth-most populous country in Europe, with a population of around 41 million people. It is also bordered by Belarus to the north; by Poland, Slovakia, and Hungary to the west; and by Romania and Moldova to the southwest; with a coastline along the Black Sea and the Sea of Azov to the south and southeast. Kyiv is the nation's capital and largest city. Ukraine's official and national language is Ukrainian; most people are also fluent in Russian.\",\n",
    "#             \"max_questions\": 5\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "            \"input_text\": ''' pipeline\n",
    "A pipeline in 🤗 Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.\n",
    "\n",
    "For more details, see Pipelines for inference.\n",
    "\n",
    "pixel values\n",
    "A tensor of the numerical representations of an image that is passed to a model. The pixel values have a shape of [batch_size, num_channels, height, width], and are generated from an image processor.\n",
    "\n",
    "pooling\n",
    "An operation that reduces a matrix into a smaller matrix, either by taking the maximum or average of the pooled dimension(s). Pooling layers are commonly found between convolutional layers to downsample the feature representation.\n",
    "\n",
    "position IDs\n",
    "Contrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of each token. Therefore, the position IDs (position_ids) are used by the model to identify each token’s position in the list of tokens.\n",
    "\n",
    "They are an optional parameter. If no position_ids are passed to the model, the IDs are automatically created as absolute positional embeddings.\n",
    "\n",
    "Absolute positional embeddings are selected in the range [0, config.max_position_embeddings - 1]. Some models use other types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings.\n",
    "\n",
    "preprocessing\n",
    "The task of preparing raw data into a format that can be easily consumed by machine learning models. For example, text is typically preprocessed by tokenization. To gain a better idea of what preprocessing looks like for other input types, check out the Preprocess tutorial.\n",
    "\n",
    "pretrained model\n",
    "A model that has been pretrained on some data (for instance all of Wikipedia). Pretraining methods involve a self-supervised objective, which can be reading the text and trying to predict the next word (see causal language modeling) or masking some words and trying to predict them (see masked language modeling).\n",
    "\n",
    "Speech and vision models have their own pretraining objectives. For example, Wav2Vec2 is a speech model pretrained on a contrastive task which requires the model to identify the “true” speech representation from a set of “false” speech representations. On the other hand, BEiT is a vision model pretrained on a masked image modeling task which masks some of the image patches and requires the model to predict the masked patches (similar to the masked language modeling objective).\n",
    "\n",
    "R\n",
    "recurrent neural network (RNN)\n",
    "A type of model that uses a loop over a layer to process texts.\n",
    "\n",
    "representation learning\n",
    "A subfield of machine learning which focuses on learning meaningful representations of raw data. Some examples of representation learning techniques include word embeddings, autoencoders, and Generative Adversarial Networks (GANs).\n",
    "\n",
    "S\n",
    "sampling rate\n",
    "A measurement in hertz of the number of samples (the audio signal) taken per second. The sampling rate is a result of discretizing a continuous signal such as speech.\n",
    "\n",
    "self-attention\n",
    "Each element of the input finds out which other elements of the input they should attend to.\n",
    "\n",
    "self-supervised learning\n",
    "A category of machine learning techniques in which a model creates its own learning objective from unlabeled data. It differs from unsupervised learning and supervised learning in that the learning process is supervised, but not explicitly from the user.\n",
    "\n",
    "One example of self-supervised learning is masked language modeling, where a model is passed sentences with a proportion of its tokens removed and learns to predict the missing tokens.\n",
    "\n",
    "semi-supervised learning\n",
    "A broad category of machine learning training techniques that leverages a small amount of labeled data with a larger quantity of unlabeled data to improve the accuracy of a model, unlike supervised learning and unsupervised learning.\n",
    "\n",
    "An example of a semi-supervised learning approach is “self-training”, in which a model is trained on labeled data, and then used to make predictions on the unlabeled data. The portion of the unlabeled data that the model predicts with the most confidence gets added to the labeled dataset and used to retrain the model.\n",
    "\n",
    "sequence-to-sequence (seq2seq)\n",
    "Models that generate a new sequence from an input, like translation models, or summarization models (such as Bart or T5).\n",
    "\n",
    "stride\n",
    "In convolution or pooling, the stride refers to the distance the kernel is moved over a matrix. A stride of 1 means the kernel is moved one pixel over at a time, and a stride of 2 means the kernel is moved two pixels over at a time.\n",
    "\n",
    "supervised learning\n",
    "A form of model training that directly uses labeled data to correct and instruct model performance. Data is fed into the model being trained, and its predictions are compared to the known labels. The model updates its weights based on how incorrect its predictions were, and the process is repeated to optimize model performance.\n",
    "\n",
    "T\n",
    "token\n",
    "A part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a punctuation symbol.\n",
    "\n",
    "token Type IDs\n",
    "Some models’ purpose is to do classification on pairs of sentences or question answering.\n",
    "\n",
    "''',\n",
    "            \"max_questions\": 3\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\torch\\serialization.py:975: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  obj = cast(Storage, torch.UntypedStorage(nbytes))\n"
     ]
    }
   ],
   "source": [
    "qg = main.QGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside get_keywords function 10\n",
      "After filter_phrases function 10\n",
      "number of keyword: 3\n",
      "Running model for generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sense2vec_distractors failed for word :  example stages\n",
      " Sense2vec_distractors failed for word :  embeddings\n",
      " Sense2vec_distractors successful for word :  pipeline\n",
      "{'questions': [{'answer': 'pipeline',\n",
      "                'context': 'pipeline\\n'\n",
      "                           'A pipeline in 🤗 Transformers is an abstraction '\n",
      "                           'referring to a series of steps that are executed '\n",
      "                           'in a specific order to preprocess and transform '\n",
      "                           'data and return a prediction from a model. '\n",
      "                           'pipeline\\n'\n",
      "                           'A pipeline in 🤗 Transformers is an abstraction '\n",
      "                           'referring to a series of steps that are executed '\n",
      "                           'in a specific order to preprocess and transform '\n",
      "                           'data and return a prediction from a model. Some '\n",
      "                           'example stages found in a pipeline might be data '\n",
      "                           'preprocessing, feature extraction, and '\n",
      "                           'normalization.',\n",
      "                'extra_options': ['Oil Sands',\n",
      "                                  'Refineries',\n",
      "                                  'Transcanada',\n",
      "                                  'Kxl'],\n",
      "                'id': 3,\n",
      "                'options': ['Tar Sands', 'Keystone Xl', 'Canadian Oil'],\n",
      "                'options_algorithm': 'sense2vec',\n",
      "                'question_statement': 'What is the abstraction referring to a '\n",
      "                                      'series of steps that are executed in a '\n",
      "                                      'specific order to preprocess and '\n",
      "                                      'transform data and return a prediction '\n",
      "                                      'from a model?',\n",
      "                'question_type': 'MCQ'}],\n",
      " 'statement': 'pipeline\\n'\n",
      "              'A pipeline in 🤗 Transformers is an abstraction referring to a '\n",
      "              'series of steps that are executed in a specific order to '\n",
      "              'preprocess and transform data and return a prediction from a '\n",
      "              'model. Some example stages found in a pipeline might be data '\n",
      "              'preprocessing, feature extraction, and normalization. For more '\n",
      "              'details, see Pipelines for inference. pixel values\\n'\n",
      "              'A tensor of the numerical representations of an image that is '\n",
      "              'passed to a model. The pixel values have a shape of '\n",
      "              '[batch_size, num_channels, height, width], and are generated '\n",
      "              'from an image processor. pooling\\n'\n",
      "              'An operation that reduces a matrix into a smaller matrix, '\n",
      "              'either by taking the maximum or average of the pooled '\n",
      "              'dimension(s). Pooling layers are commonly found between '\n",
      "              'convolutional layers to downsample the feature representation. '\n",
      "              'position IDs\\n'\n",
      "              'Contrary to RNNs that have the position of each token embedded '\n",
      "              'within them, transformers are unaware of the position of each '\n",
      "              'token. Therefore, the position IDs (position_ids) are used by '\n",
      "              'the model to identify each token’s position in the list of '\n",
      "              'tokens. They are an optional parameter. If no position_ids are '\n",
      "              'passed to the model, the IDs are automatically created as '\n",
      "              'absolute positional embeddings. Absolute positional embeddings '\n",
      "              'are selected in the range [0, config.max_position_embeddings - '\n",
      "              '1]. Some models use other types of positional embeddings, such '\n",
      "              'as sinusoidal position embeddings or relative position '\n",
      "              'embeddings. preprocessing\\n'\n",
      "              'The task of preparing raw data into a format that can be easily '\n",
      "              'consumed by machine learning models. For example, text is '\n",
      "              'typically preprocessed by tokenization. To gain a better idea '\n",
      "              'of what preprocessing looks like for other input types, check '\n",
      "              'out the Preprocess tutorial. pretrained model\\n'\n",
      "              'A model that has been pretrained on some data (for instance all '\n",
      "              'of Wikipedia). Pretraining methods involve a self-supervised '\n",
      "              'objective, which can be reading the text and trying to predict '\n",
      "              'the next word (see causal language modeling) or masking some '\n",
      "              'words and trying to predict them (see masked language '\n",
      "              'modeling). Speech and vision models have their own pretraining '\n",
      "              'objectives. For example, Wav2Vec2 is a speech model pretrained '\n",
      "              'on a contrastive task which requires the model to identify the '\n",
      "              '“true” speech representation from a set of “false” speech '\n",
      "              'representations. On the other hand, BEiT is a vision model '\n",
      "              'pretrained on a masked image modeling task which masks some of '\n",
      "              'the image patches and requires the model to predict the masked '\n",
      "              'patches (similar to the masked language modeling objective). R\\n'\n",
      "              'recurrent neural network (RNN)\\n'\n",
      "              'A type of model that uses a loop over a layer to process texts. '\n",
      "              'representation learning\\n'\n",
      "              'A subfield of machine learning which focuses on learning '\n",
      "              'meaningful representations of raw data. Some examples of '\n",
      "              'representation learning techniques include word embeddings, '\n",
      "              'autoencoders, and Generative Adversarial Networks (GANs). S\\n'\n",
      "              'sampling rate\\n'\n",
      "              'A measurement in hertz of the number of samples (the audio '\n",
      "              'signal) taken per second. The sampling rate is a result of '\n",
      "              'discretizing a continuous signal such as speech. '\n",
      "              'self-attention\\n'\n",
      "              'Each element of the input finds out which other elements of the '\n",
      "              'input they should attend to. self-supervised learning\\n'\n",
      "              'A category of machine learning techniques in which a model '\n",
      "              'creates its own learning objective from unlabeled data. It '\n",
      "              'differs from unsupervised learning and supervised learning in '\n",
      "              'that the learning process is supervised, but not explicitly '\n",
      "              'from the user. One example of self-supervised learning is '\n",
      "              'masked language modeling, where a model is passed sentences '\n",
      "              'with a proportion of its tokens removed and learns to predict '\n",
      "              'the missing tokens. semi-supervised learning\\n'\n",
      "              'A broad category of machine learning training techniques that '\n",
      "              'leverages a small amount of labeled data with a larger quantity '\n",
      "              'of unlabeled data to improve the accuracy of a model, unlike '\n",
      "              'supervised learning and unsupervised learning. An example of a '\n",
      "              'semi-supervised learning approach is “self-training”, in which '\n",
      "              'a model is trained on labeled data, and then used to make '\n",
      "              'predictions on the unlabeled data. The portion of the unlabeled '\n",
      "              'data that the model predicts with the most confidence gets '\n",
      "              'added to the labeled dataset and used to retrain the model. '\n",
      "              'sequence-to-sequence (seq2seq)\\n'\n",
      "              'Models that generate a new sequence from an input, like '\n",
      "              'translation models, or summarization models (such as Bart or '\n",
      "              'T5). stride\\n'\n",
      "              'In convolution or pooling, the stride refers to the distance '\n",
      "              'the kernel is moved over a matrix. A stride of 1 means the '\n",
      "              'kernel is moved one pixel over at a time, and a stride of 2 '\n",
      "              'means the kernel is moved two pixels over at a time. supervised '\n",
      "              'learning\\n'\n",
      "              'A form of model training that directly uses labeled data to '\n",
      "              'correct and instruct model performance. Data is fed into the '\n",
      "              'model being trained, and its predictions are compared to the '\n",
      "              'known labels. The model updates its weights based on how '\n",
      "              'incorrect its predictions were, and the process is repeated to '\n",
      "              'optimize model performance. T\\n'\n",
      "              'token\\n'\n",
      "              'A part of a sentence, usually a word, but can also be a subword '\n",
      "              '(non-common words are often split in subwords) or a punctuation '\n",
      "              'symbol. token Type IDs\\n'\n",
      "              'Some models’ purpose is to do classification on pairs of '\n",
      "              'sentences or question answering.',\n",
      " 'time_taken': 20.283108949661255}\n"
     ]
    }
   ],
   "source": [
    "output = qg.predict_mcq(payload, num_options = 3)\n",
    "pprint (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate FAQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside get_keywords function 10\n",
      "After filter_phrases function 10\n",
      "Running model for generation\n",
      "{'questions': [{'Question': 'Data preprocessing, feature extraction, and normalization are some of the examples of what?', 'Answer': 'example stages', 'id': 1, 'context': 'Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.'}, {'Question': 'What are some examples of representation learning techniques?', 'Answer': 'embeddings', 'id': 2, 'context': 'Some examples of representation learning techniques include word embeddings, autoencoders, and Generative Adversarial Networks (GANs). Some models use other types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings. Some models use other types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings.'}, {'Question': 'What is the abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model?', 'Answer': 'pipeline', 'id': 3, 'context': 'pipeline\\nA pipeline in 🤗 Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. pipeline\\nA pipeline in 🤗 Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.'}, {'Question': 'What is the position of each token embedded within a RNN?', 'Answer': 'token', 'id': 4, 'context': 'position IDs\\nContrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of each token. position IDs\\nContrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of each token. T\\ntoken\\nA part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a punctuation symbol.'}, {'Question': 'What is the subfield of machine learning which focuses on learning meaningful representations of raw data?', 'Answer': 'representations', 'id': 5, 'context': 'For example, Wav2Vec2 is a speech model pretrained on a contrastive task which requires the model to identify the “true” speech representation from a set of “false” speech representations. representation learning\\nA subfield of machine learning which focuses on learning meaningful representations of raw data. pixel values\\nA tensor of the numerical representations of an image that is passed to a model.'}, {'Question': 'What is the term for machine learning that leverages a small amount of labeled data with a larger quantity of unlabeled data to improve the accuracy of a model?', 'Answer': 'learning', 'id': 6, 'context': 'semi-supervised learning\\nA broad category of machine learning training techniques that leverages a small amount of labeled data with a larger quantity of unlabeled data to improve the accuracy of a model, unlike supervised learning and unsupervised learning. semi-supervised learning\\nA broad category of machine learning training techniques that leverages a small amount of labeled data with a larger quantity of unlabeled data to improve the accuracy of a model, unlike supervised learning and unsupervised learning. semi-supervised learning\\nA broad category of machine learning training techniques that leverages a small amount of labeled data with a larger quantity of unlabeled data to improve the accuracy of a model, unlike supervised learning and unsupervised learning.'}, {'Question': 'What is the BEiT?', 'Answer': 'model', 'id': 7, 'context': 'semi-supervised learning\\nA broad category of machine learning training techniques that leverages a small amount of labeled data with a larger quantity of unlabeled data to improve the accuracy of a model, unlike supervised learning and unsupervised learning. On the other hand, BEiT is a vision model pretrained on a masked image modeling task which masks some of the image patches and requires the model to predict the masked patches (similar to the masked language modeling objective). On the other hand, BEiT is a vision model pretrained on a masked image modeling task which masks some of the image patches and requires the model to predict the masked patches (similar to the masked language modeling objective).'}, {'Question': 'What is used to improve the accuracy of a model?', 'Answer': 'data', 'id': 8, 'context': 'semi-supervised learning\\nA broad category of machine learning training techniques that leverages a small amount of labeled data with a larger quantity of unlabeled data to improve the accuracy of a model, unlike supervised learning and unsupervised learning. semi-supervised learning\\nA broad category of machine learning training techniques that leverages a small amount of labeled data with a larger quantity of unlabeled data to improve the accuracy of a model, unlike supervised learning and unsupervised learning. pipeline\\nA pipeline in 🤗 Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model.'}, {'Question': 'What is the position of each token embedded within a RNN?', 'Answer': 'position', 'id': 9, 'context': 'position IDs\\nContrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of each token. position IDs\\nContrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of each token. position IDs\\nContrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of each token.'}, {'Question': 'What is a semi-supervised learning approach called?', 'Answer': 'example', 'id': 10, 'context': 'For example, Wav2Vec2 is a speech model pretrained on a contrastive task which requires the model to identify the “true” speech representation from a set of “false” speech representations. One example of self-supervised learning is masked language modeling, where a model is passed sentences with a proportion of its tokens removed and learns to predict the missing tokens. An example of a semi-supervised learning approach is “self-training”, in which a model is trained on labeled data, and then used to make predictions on the unlabeled data.'}]}\n",
      "{'questions': [{'Answer': 'example stages',\n",
      "                'Question': 'Data preprocessing, feature extraction, and '\n",
      "                            'normalization are some of the examples of what?',\n",
      "                'context': 'Some example stages found in a pipeline might be '\n",
      "                           'data preprocessing, feature extraction, and '\n",
      "                           'normalization.',\n",
      "                'id': 1},\n",
      "               {'Answer': 'embeddings',\n",
      "                'Question': 'What are some examples of representation learning '\n",
      "                            'techniques?',\n",
      "                'context': 'Some examples of representation learning '\n",
      "                           'techniques include word embeddings, autoencoders, '\n",
      "                           'and Generative Adversarial Networks (GANs). Some '\n",
      "                           'models use other types of positional embeddings, '\n",
      "                           'such as sinusoidal position embeddings or relative '\n",
      "                           'position embeddings. Some models use other types '\n",
      "                           'of positional embeddings, such as sinusoidal '\n",
      "                           'position embeddings or relative position '\n",
      "                           'embeddings.',\n",
      "                'id': 2},\n",
      "               {'Answer': 'pipeline',\n",
      "                'Question': 'What is the abstraction referring to a series of '\n",
      "                            'steps that are executed in a specific order to '\n",
      "                            'preprocess and transform data and return a '\n",
      "                            'prediction from a model?',\n",
      "                'context': 'pipeline\\n'\n",
      "                           'A pipeline in 🤗 Transformers is an abstraction '\n",
      "                           'referring to a series of steps that are executed '\n",
      "                           'in a specific order to preprocess and transform '\n",
      "                           'data and return a prediction from a model. '\n",
      "                           'pipeline\\n'\n",
      "                           'A pipeline in 🤗 Transformers is an abstraction '\n",
      "                           'referring to a series of steps that are executed '\n",
      "                           'in a specific order to preprocess and transform '\n",
      "                           'data and return a prediction from a model. Some '\n",
      "                           'example stages found in a pipeline might be data '\n",
      "                           'preprocessing, feature extraction, and '\n",
      "                           'normalization.',\n",
      "                'id': 3},\n",
      "               {'Answer': 'token',\n",
      "                'Question': 'What is the position of each token embedded '\n",
      "                            'within a RNN?',\n",
      "                'context': 'position IDs\\n'\n",
      "                           'Contrary to RNNs that have the position of each '\n",
      "                           'token embedded within them, transformers are '\n",
      "                           'unaware of the position of each token. position '\n",
      "                           'IDs\\n'\n",
      "                           'Contrary to RNNs that have the position of each '\n",
      "                           'token embedded within them, transformers are '\n",
      "                           'unaware of the position of each token. T\\n'\n",
      "                           'token\\n'\n",
      "                           'A part of a sentence, usually a word, but can also '\n",
      "                           'be a subword (non-common words are often split in '\n",
      "                           'subwords) or a punctuation symbol.',\n",
      "                'id': 4},\n",
      "               {'Answer': 'representations',\n",
      "                'Question': 'What is the subfield of machine learning which '\n",
      "                            'focuses on learning meaningful representations of '\n",
      "                            'raw data?',\n",
      "                'context': 'For example, Wav2Vec2 is a speech model pretrained '\n",
      "                           'on a contrastive task which requires the model to '\n",
      "                           'identify the “true” speech representation from a '\n",
      "                           'set of “false” speech representations. '\n",
      "                           'representation learning\\n'\n",
      "                           'A subfield of machine learning which focuses on '\n",
      "                           'learning meaningful representations of raw data. '\n",
      "                           'pixel values\\n'\n",
      "                           'A tensor of the numerical representations of an '\n",
      "                           'image that is passed to a model.',\n",
      "                'id': 5},\n",
      "               {'Answer': 'learning',\n",
      "                'Question': 'What is the term for machine learning that '\n",
      "                            'leverages a small amount of labeled data with a '\n",
      "                            'larger quantity of unlabeled data to improve the '\n",
      "                            'accuracy of a model?',\n",
      "                'context': 'semi-supervised learning\\n'\n",
      "                           'A broad category of machine learning training '\n",
      "                           'techniques that leverages a small amount of '\n",
      "                           'labeled data with a larger quantity of unlabeled '\n",
      "                           'data to improve the accuracy of a model, unlike '\n",
      "                           'supervised learning and unsupervised learning. '\n",
      "                           'semi-supervised learning\\n'\n",
      "                           'A broad category of machine learning training '\n",
      "                           'techniques that leverages a small amount of '\n",
      "                           'labeled data with a larger quantity of unlabeled '\n",
      "                           'data to improve the accuracy of a model, unlike '\n",
      "                           'supervised learning and unsupervised learning. '\n",
      "                           'semi-supervised learning\\n'\n",
      "                           'A broad category of machine learning training '\n",
      "                           'techniques that leverages a small amount of '\n",
      "                           'labeled data with a larger quantity of unlabeled '\n",
      "                           'data to improve the accuracy of a model, unlike '\n",
      "                           'supervised learning and unsupervised learning.',\n",
      "                'id': 6},\n",
      "               {'Answer': 'model',\n",
      "                'Question': 'What is the BEiT?',\n",
      "                'context': 'semi-supervised learning\\n'\n",
      "                           'A broad category of machine learning training '\n",
      "                           'techniques that leverages a small amount of '\n",
      "                           'labeled data with a larger quantity of unlabeled '\n",
      "                           'data to improve the accuracy of a model, unlike '\n",
      "                           'supervised learning and unsupervised learning. On '\n",
      "                           'the other hand, BEiT is a vision model pretrained '\n",
      "                           'on a masked image modeling task which masks some '\n",
      "                           'of the image patches and requires the model to '\n",
      "                           'predict the masked patches (similar to the masked '\n",
      "                           'language modeling objective). On the other hand, '\n",
      "                           'BEiT is a vision model pretrained on a masked '\n",
      "                           'image modeling task which masks some of the image '\n",
      "                           'patches and requires the model to predict the '\n",
      "                           'masked patches (similar to the masked language '\n",
      "                           'modeling objective).',\n",
      "                'id': 7},\n",
      "               {'Answer': 'data',\n",
      "                'Question': 'What is used to improve the accuracy of a model?',\n",
      "                'context': 'semi-supervised learning\\n'\n",
      "                           'A broad category of machine learning training '\n",
      "                           'techniques that leverages a small amount of '\n",
      "                           'labeled data with a larger quantity of unlabeled '\n",
      "                           'data to improve the accuracy of a model, unlike '\n",
      "                           'supervised learning and unsupervised learning. '\n",
      "                           'semi-supervised learning\\n'\n",
      "                           'A broad category of machine learning training '\n",
      "                           'techniques that leverages a small amount of '\n",
      "                           'labeled data with a larger quantity of unlabeled '\n",
      "                           'data to improve the accuracy of a model, unlike '\n",
      "                           'supervised learning and unsupervised learning. '\n",
      "                           'pipeline\\n'\n",
      "                           'A pipeline in 🤗 Transformers is an abstraction '\n",
      "                           'referring to a series of steps that are executed '\n",
      "                           'in a specific order to preprocess and transform '\n",
      "                           'data and return a prediction from a model.',\n",
      "                'id': 8},\n",
      "               {'Answer': 'position',\n",
      "                'Question': 'What is the position of each token embedded '\n",
      "                            'within a RNN?',\n",
      "                'context': 'position IDs\\n'\n",
      "                           'Contrary to RNNs that have the position of each '\n",
      "                           'token embedded within them, transformers are '\n",
      "                           'unaware of the position of each token. position '\n",
      "                           'IDs\\n'\n",
      "                           'Contrary to RNNs that have the position of each '\n",
      "                           'token embedded within them, transformers are '\n",
      "                           'unaware of the position of each token. position '\n",
      "                           'IDs\\n'\n",
      "                           'Contrary to RNNs that have the position of each '\n",
      "                           'token embedded within them, transformers are '\n",
      "                           'unaware of the position of each token.',\n",
      "                'id': 9},\n",
      "               {'Answer': 'example',\n",
      "                'Question': 'What is a semi-supervised learning approach '\n",
      "                            'called?',\n",
      "                'context': 'For example, Wav2Vec2 is a speech model pretrained '\n",
      "                           'on a contrastive task which requires the model to '\n",
      "                           'identify the “true” speech representation from a '\n",
      "                           'set of “false” speech representations. One example '\n",
      "                           'of self-supervised learning is masked language '\n",
      "                           'modeling, where a model is passed sentences with a '\n",
      "                           'proportion of its tokens removed and learns to '\n",
      "                           'predict the missing tokens. An example of a '\n",
      "                           'semi-supervised learning approach is '\n",
      "                           '“self-training”, in which a model is trained on '\n",
      "                           'labeled data, and then used to make predictions on '\n",
      "                           'the unlabeled data.',\n",
      "                'id': 10}],\n",
      " 'statement': 'pipeline\\n'\n",
      "              'A pipeline in 🤗 Transformers is an abstraction referring to a '\n",
      "              'series of steps that are executed in a specific order to '\n",
      "              'preprocess and transform data and return a prediction from a '\n",
      "              'model. Some example stages found in a pipeline might be data '\n",
      "              'preprocessing, feature extraction, and normalization. For more '\n",
      "              'details, see Pipelines for inference. pixel values\\n'\n",
      "              'A tensor of the numerical representations of an image that is '\n",
      "              'passed to a model. The pixel values have a shape of '\n",
      "              '[batch_size, num_channels, height, width], and are generated '\n",
      "              'from an image processor. pooling\\n'\n",
      "              'An operation that reduces a matrix into a smaller matrix, '\n",
      "              'either by taking the maximum or average of the pooled '\n",
      "              'dimension(s). Pooling layers are commonly found between '\n",
      "              'convolutional layers to downsample the feature representation. '\n",
      "              'position IDs\\n'\n",
      "              'Contrary to RNNs that have the position of each token embedded '\n",
      "              'within them, transformers are unaware of the position of each '\n",
      "              'token. Therefore, the position IDs (position_ids) are used by '\n",
      "              'the model to identify each token’s position in the list of '\n",
      "              'tokens. They are an optional parameter. If no position_ids are '\n",
      "              'passed to the model, the IDs are automatically created as '\n",
      "              'absolute positional embeddings. Absolute positional embeddings '\n",
      "              'are selected in the range [0, config.max_position_embeddings - '\n",
      "              '1]. Some models use other types of positional embeddings, such '\n",
      "              'as sinusoidal position embeddings or relative position '\n",
      "              'embeddings. preprocessing\\n'\n",
      "              'The task of preparing raw data into a format that can be easily '\n",
      "              'consumed by machine learning models. For example, text is '\n",
      "              'typically preprocessed by tokenization. To gain a better idea '\n",
      "              'of what preprocessing looks like for other input types, check '\n",
      "              'out the Preprocess tutorial. pretrained model\\n'\n",
      "              'A model that has been pretrained on some data (for instance all '\n",
      "              'of Wikipedia). Pretraining methods involve a self-supervised '\n",
      "              'objective, which can be reading the text and trying to predict '\n",
      "              'the next word (see causal language modeling) or masking some '\n",
      "              'words and trying to predict them (see masked language '\n",
      "              'modeling). Speech and vision models have their own pretraining '\n",
      "              'objectives. For example, Wav2Vec2 is a speech model pretrained '\n",
      "              'on a contrastive task which requires the model to identify the '\n",
      "              '“true” speech representation from a set of “false” speech '\n",
      "              'representations. On the other hand, BEiT is a vision model '\n",
      "              'pretrained on a masked image modeling task which masks some of '\n",
      "              'the image patches and requires the model to predict the masked '\n",
      "              'patches (similar to the masked language modeling objective). R\\n'\n",
      "              'recurrent neural network (RNN)\\n'\n",
      "              'A type of model that uses a loop over a layer to process texts. '\n",
      "              'representation learning\\n'\n",
      "              'A subfield of machine learning which focuses on learning '\n",
      "              'meaningful representations of raw data. Some examples of '\n",
      "              'representation learning techniques include word embeddings, '\n",
      "              'autoencoders, and Generative Adversarial Networks (GANs). S\\n'\n",
      "              'sampling rate\\n'\n",
      "              'A measurement in hertz of the number of samples (the audio '\n",
      "              'signal) taken per second. The sampling rate is a result of '\n",
      "              'discretizing a continuous signal such as speech. '\n",
      "              'self-attention\\n'\n",
      "              'Each element of the input finds out which other elements of the '\n",
      "              'input they should attend to. self-supervised learning\\n'\n",
      "              'A category of machine learning techniques in which a model '\n",
      "              'creates its own learning objective from unlabeled data. It '\n",
      "              'differs from unsupervised learning and supervised learning in '\n",
      "              'that the learning process is supervised, but not explicitly '\n",
      "              'from the user. One example of self-supervised learning is '\n",
      "              'masked language modeling, where a model is passed sentences '\n",
      "              'with a proportion of its tokens removed and learns to predict '\n",
      "              'the missing tokens. semi-supervised learning\\n'\n",
      "              'A broad category of machine learning training techniques that '\n",
      "              'leverages a small amount of labeled data with a larger quantity '\n",
      "              'of unlabeled data to improve the accuracy of a model, unlike '\n",
      "              'supervised learning and unsupervised learning. An example of a '\n",
      "              'semi-supervised learning approach is “self-training”, in which '\n",
      "              'a model is trained on labeled data, and then used to make '\n",
      "              'predictions on the unlabeled data. The portion of the unlabeled '\n",
      "              'data that the model predicts with the most confidence gets '\n",
      "              'added to the labeled dataset and used to retrain the model. '\n",
      "              'sequence-to-sequence (seq2seq)\\n'\n",
      "              'Models that generate a new sequence from an input, like '\n",
      "              'translation models, or summarization models (such as Bart or '\n",
      "              'T5). stride\\n'\n",
      "              'In convolution or pooling, the stride refers to the distance '\n",
      "              'the kernel is moved over a matrix. A stride of 1 means the '\n",
      "              'kernel is moved one pixel over at a time, and a stride of 2 '\n",
      "              'means the kernel is moved two pixels over at a time. supervised '\n",
      "              'learning\\n'\n",
      "              'A form of model training that directly uses labeled data to '\n",
      "              'correct and instruct model performance. Data is fed into the '\n",
      "              'model being trained, and its predictions are compared to the '\n",
      "              'known labels. The model updates its weights based on how '\n",
      "              'incorrect its predictions were, and the process is repeated to '\n",
      "              'optimize model performance. T\\n'\n",
      "              'token\\n'\n",
      "              'A part of a sentence, usually a word, but can also be a subword '\n",
      "              '(non-common words are often split in subwords) or a punctuation '\n",
      "              'symbol. token Type IDs\\n'\n",
      "              'Some models’ purpose is to do classification on pairs of '\n",
      "              'sentences or question answering.'}\n"
     ]
    }
   ],
   "source": [
    "output = qg.predict_shortq(payload)\n",
    "pprint (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 479kB/s]\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\quiz_gen\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 242M/242M [00:22<00:00, 11.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "answer = main.AnswerPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload3 = {\n",
    "    \"input_text\" : '''Sachin Ramesh Tendulkar is a former international cricketer from \n",
    "              India and a former captain of the Indian national team. He is widely regarded \n",
    "              as one of the greatest batsmen in the history of cricket. He is the highest\n",
    "               run scorer of all time in International cricket.''',\n",
    "    \"input_question\" : \"Who is Sachin tendulkar ? \"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sachin ramesh tendulkar is a former international cricketer from india and a former captain of the indian national team.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.predict_answer(payload3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
